{
 "metadata": {
  "name": "",
  "signature": "sha256:8e971093fb9bca5e6c5cb86492467c39a2b8b22ce8bd4c69971d655172116583"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see from the documentation of LingPy but also from the publications devoted to LingPy, the LexStat algorithm builds its own correspondence scorer, based on a simplified model of context. Since both the creation of the scorer and the handling of the context often create confusion among the users of LingPy, it seems worthwhile to illustrate how it works internally, and how you can use LingPy to make the calculations visible. In the following tutorial, we will:\n",
      "\n",
      "* carry out a rudimentary cognate detection analysis with LexStat\n",
      "* inspect the internal conversion to sound classes and to prosodic strings which are being used to handle the context, and\n",
      "* learn how to print investigate the scoring function created by the LexStat algorithm\n",
      "\n",
      "Lets start by simply loading one of the test files in the LingPy library. Essentially, we follow the workflow example on the website at http://lingpy.org/tutorial/workflow.html. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lingpy import *\n",
      "from lingpy.tests.util import test_data # this gives us the path to the file by Kessler (2001)\n",
      "lex = LexStat(test_data('KSL.qlc'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We quickly check the coverage, the language names, and the number of concepts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('File has {0} concepts and {1} languages.'.format(lex.height, lex.width))\n",
      "for k, v in lex.coverage().items():\n",
      "    print('Language {0:15} has {1} words'.format(k, v))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "File has 200 concepts and 7 languages.\n",
        "Language English         has 200 words\n",
        "Language Navajo          has 200 words\n",
        "Language Turkish         has 200 words\n",
        "Language Albanian        has 200 words\n",
        "Language German          has 200 words\n",
        "Language Hawaiian        has 200 words\n",
        "Language French          has 200 words\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When loading the data into LexStat, LingPy automatically creates different representations of the original strings. Thus, for a transcription that is added in unsegmentized form, LingPy will create\n",
      "\n",
      "* a segmented representation, stored in column with the name `lex._segments` (defaults to \"tokens\")\n",
      "* a sound class representation, stored in column `lex._classes` (defaults to \"classes\")\n",
      "* a prosodic string representation, stored in column `lex._prostring` (defaults to \"prostring\")\n",
      "* a unique identifier representation by which each segment is identified by a tuple consisting of a numberic ID for the language, the sound-class value, and the prosodic-string, stored in column `lex._numbers` (defaults to \"numbers\")\n",
      "* intermediate values to create the prosodic string and the identifiers for each segment in each language\n",
      "\n",
      "Let us now only take the values in one language and see how the different values are represented. As a general rule, most of the values are list-types."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = [k for k in lex][:3] # we take the first 5 words\n",
      "for k in words:\n",
      "    print('{0} [{1}] \u00ab{2}\u00bb'.format(lex[k, 'language'], lex[k, 'ipa'], lex[k, 'concept'])) # original transcription\n",
      "    for h in [\"tokens\", \"classes\", \"prostrings\", \"numbers\"]:\n",
      "        print('{0:20}'.format(h), '   '.join(['{0:5}'.format(s) for s in lex[k, h]]))\n",
      "    print('---')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Albanian [\u025fi\u03b8] \u00aball\u00bb\n",
        "tokens               \u025f       i       \u03b8    \n",
        "classes              C       I       D    \n",
        "prostrings           A       X       N    \n",
        "numbers              1.C.C   1.I.V   1.D.c\n",
        "---\n",
        "English [\u0254l] \u00aball\u00bb\n",
        "tokens               \u0254       l    \n",
        "classes              U       L    \n",
        "prostrings           X       N    \n",
        "numbers              2.U.V   2.L.c\n",
        "---\n",
        "French [tut] \u00aball\u00bb\n",
        "tokens               t       u       t    \n",
        "classes              T       Y       T    \n",
        "prostrings           A       X       N    \n",
        "numbers              3.T.C   3.Y.V   3.T.c\n",
        "---\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this example, we can see exactly how the values are converted. While the conversion to sound classes is determined by the keyword `model` that we can pass to LexStat when initializing a LexStat object, the prosodic string can be customized by passing our own function to create a prosodic string out of a segmented IPA string with the keyword `get_prostring`. Prosodic strings are a bit complicated to interpret, and it is important to read the information at http://lingpy.org thoroughly. In concrete, we currently use the letters to define the following values:\n",
      "\n",
      "    A: sequence-initial consonant\n",
      "    B: syllable-initial, non-sequence initial consonant in a context of ascending sonority\n",
      "    C: non-syllable, non-initial consonant in ascending sonority context\n",
      "    L: non-syllable-final consonant in descending environment\n",
      "    M: syllable-final consonant in descending environment\n",
      "    N: word-final consonant\n",
      "    X: first vowel in a word\n",
      "    Y: non-final vowel in a word\n",
      "    Z: vowel occuring in the last position of a word\n",
      "    _: word break\n",
      "    \n",
      "For our first string (Albanian) above, which is represented as `AXN`, this means that the first sound is a sequence-initial consonant (`A`), the second is the first vowel in a word (`X`), and the third is a word-final consonant (`N`). \n",
      "\n",
      "The last value in the row, the \"numbers\", consist of three different values that are used to define each segment in the data. The first element is an ID representing the language in which this segment occurs (remember: LexStat works language-dependent regarding the scores), the second ekement is the sound class value, and the third is a simplified form of the prosodic string, in which less values are distinguished than in the original prosodic string calculations. This is important when dealing with sparse data, as otherwise, many sounds would only occur one time, rendering all comparison futile. You can check these values like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lex._transform"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "{'A': 'C',\n",
        " 'M': 'c',\n",
        " 'X': 'V',\n",
        " 'N': 'c',\n",
        " 'T': 'T',\n",
        " 'C': 'C',\n",
        " 'B': 'C',\n",
        " 'L': 'c',\n",
        " 'Y': 'V',\n",
        " '_': '_',\n",
        " 'Z': 'V'}"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is important is that all calculataions in LexStat are carried out with the \"numbers\"! So all values for the correspondences, etc., will all be computed for these values. When loading the data into the LexStat object, initial statistics on the frequency of the sounds are already calcuated automatically, and we can check them like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k, v in sorted(lex.freqs['German'].items(), key=lambda x: x[0]):\n",
      "    if k[2] not in 'AEIOUY':\n",
      "        print('{0[1]} / {0[2]}'.format(k.split('.')), v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "B / C 48\n",
        "B / c 7\n",
        "C / C 8\n",
        "C / c 6\n",
        "G / C 1\n",
        "G / c 13\n",
        "H / C 12\n",
        "J / C 2\n",
        "K / C 30\n",
        "K / c 17\n",
        "L / C 22\n",
        "L / c 23\n",
        "M / C 12\n",
        "M / c 8\n",
        "N / C 19\n",
        "N / c 45\n",
        "P / C 17\n",
        "P / c 6\n",
        "R / C 25\n",
        "R / c 36\n",
        "S / C 42\n",
        "S / c 23\n",
        "T / C 34\n",
        "T / c 33\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that thanks to the prosodic strings, we have two different kinds of sounds for all consonants here: one in pre-nucleic position (syllable-initial or before the main vowel), one in coda-position (after main vowel). This will be important when interpreting the sound correspondences afterwards. In order to investigate these, let us now create the scorer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lex.get_scorer(runs=10000, force=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "CORRESPONDENCE CALCULATION:   0%|          | 0/24.5 [00:00<?, ?it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "CORRESPONDENCE CALCULATION:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 15/24.5 [00:00<00:00, 141.79it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "                                                                              "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:   0%|          | 0/24.5 [00:00<?, ?it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:   8%|\u258a         | 2/24.5 [00:00<00:03,  6.47it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  12%|\u2588\u258f        | 3/24.5 [00:00<00:04,  5.10it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  16%|\u2588\u258b        | 4/24.5 [00:00<00:04,  4.40it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  20%|\u2588\u2588        | 5/24.5 [00:01<00:04,  3.99it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  24%|\u2588\u2588\u258d       | 6/24.5 [00:01<00:05,  3.62it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  29%|\u2588\u2588\u258a       | 7/24.5 [00:01<00:05,  3.38it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  33%|\u2588\u2588\u2588\u258e      | 8/24.5 [00:02<00:04,  3.31it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  37%|\u2588\u2588\u2588\u258b      | 9/24.5 [00:02<00:04,  3.39it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  41%|\u2588\u2588\u2588\u2588      | 10/24.5 [00:02<00:04,  3.41it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  45%|\u2588\u2588\u2588\u2588\u258d     | 11/24.5 [00:03<00:04,  3.34it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  49%|\u2588\u2588\u2588\u2588\u2589     | 12/24.5 [00:03<00:03,  3.21it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 13/24.5 [00:03<00:03,  3.18it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 14/24.5 [00:04<00:03,  3.21it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 15/24.5 [00:04<00:02,  3.27it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 16/24.5 [00:04<00:02,  3.30it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 17/24.5 [00:04<00:02,  3.23it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 18/24.5 [00:05<00:02,  3.17it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 19/24.5 [00:05<00:01,  3.14it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 20/24.5 [00:05<00:01,  3.20it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 21/24.5 [00:06<00:01,  3.15it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 22/24.5 [00:06<00:00,  3.09it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 23/24.5 [00:06<00:00,  3.13it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 24/24.5 [00:07<00:00,  2.97it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION: 25it [00:07,  2.86it/s]                          "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION: 26it [00:07,  2.86it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION: 27it [00:08,  2.78it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "RANDOM CORRESPONDENCE CALCULATION: 28it [00:08,  2.81it/s]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r",
        "                                                          "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\r"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now compare the sounds in two languages, like, say, German and English, for the values which the scorer contains for them. For this, we use the `lex.freqs` attribute to get the sound values (remember, sounds are represented as \"numbers\"), and we select only specific sounds, for the sake of visibility, as before, where vowels were excluded, in order to avoid that our matrix becomes too big. We use the tabulate package to represent the matrix, as this will save time in plotting it. Our choice of sounds are \"CKGPBTD\", so we exclude vowels and nasals, and keep velars, dentals, and labials, both as fricative and plosive. We also do not show negative values in the matrix, but only positive ones, in order to increase visibility."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tabulate\n",
      "german = sorted([k for k in lex.freqs['German'] if k[2] in 'KGBPDTCS'])\n",
      "english = sorted([k for k in lex.freqs['English'] if k[2] in 'KGPBDTCS'])\n",
      "corrs = [[''] + ['{0[1]}/{0[2]}'.format(s.split('.')) for s in german]]\n",
      "for soundA in english:\n",
      "    corrs += [['{0[1]}/{0[2]}'.format(soundA.split('.'))]]\n",
      "    for soundB in german:\n",
      "        val = lex.cscorer[soundA, soundB]\n",
      "        if round(val) <= 0:\n",
      "            val = ''\n",
      "        else:\n",
      "            val = '{0:.0f}'.format(val)\n",
      "        corrs[-1] += [val]\n",
      "print(tabulate.tabulate(corrs, tablefmt='grid'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "|     | B/C | B/c | C/C | C/c | G/C | G/c | K/C | K/c | P/C | P/c | S/C | S/c | T/C | T/c |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| B/C | 6   |     |     |     |     |     |     |     | 3   |     |     |     |     |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| B/c |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| C/C |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| C/c |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| D/C |     |     |     |     |     |     |     |     |     |     |     |     | 6   |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| D/c |     |     |     |     |     |     |     |     |     |     |     |     |     | 5   |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| K/C |     |     |     |     |     |     | 6   |     |     |     |     |     |     |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| K/c |     |     |     |     |     |     |     | 6   |     |     |     |     |     |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| P/C | 1   |     |     |     |     |     |     |     | 7   |     |     |     |     |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| P/c |     | 6   |     |     |     |     |     |     |     |     |     |     |     |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| S/C |     |     |     |     |     |     | 1   |     |     |     | 6   |     |     |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| S/c |     |     |     |     |     |     |     |     |     |     | 5   | 7   |     |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| T/C |     |     | 5   |     |     |     |     |     |     |     |     |     | 5   |     |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
        "| T/c |     |     |     | 4   |     |     |     | 1   |     |     |     | 2   | 1   | 6   |\n",
        "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see, that the similarities are really sparse, and we can also see that we have some very striking differences with respect to the similarities based on the position. For the sound-class B, covering sounds like \"f\", and \"v\", for example, we find correspondences only in word-initial position (scoring value 6, B/C means syllable-initial). But syllable-initial, we find English's sound class \"P\" matching much more strongly with German's class \"B\" (\"B/c\" in German and \"P/c\" in English). The correspondences captured here are the ones after the second consonant shift, where German turned \"p\" to \"f\", while English preserves the \"p\"."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}